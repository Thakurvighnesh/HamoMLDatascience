{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.model_selection import train_test_split\nimport csv\nimport cv2\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the train classes that contains the image names and tags from the directory\ndf_train = pd.read_csv('/kaggle/input/planet-understanding-the-amazon-from-space/train_v2.csv/train_v2.csv')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#define a function to split the tags and store a set of the tags in a variable called labels.\n#set is used to return the unique labels in the tags\nlabels = set()\ndef splitting_tags(tags):\n    for tag in tags.split():\n        labels.add(tag)\n\n#we redefine the train_classes by creating a copy of it so as not to overwrite the existing one. \n#so a copy of the train classes is stored in the variable train_classes1, we convert labels which is a set to a list.\ntrain_classes1 = df_train.copy()\ntrain_classes1['tags'].apply(splitting_tags)\nlabels = list(labels)\nprint(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assert  that the length of the dataframe is the same as the shape\nassert len(train_classes1['image_name'].unique()) == train_classes1.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##One hot encoding is performed on the labels in train classes\nfor tag in labels:\n    train_classes1[tag] = train_classes1['tags'].apply(lambda x: 1 if tag in x.split() else 0)\n    \n## adding .jpg extension to the column image_name so as to have same name format as the image files\ntrain_classes1['image_name'] = train_classes1['image_name'].apply(lambda x: '{}.jpg'.format(x))\ntrain_classes1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing tensorflow libraries for training the dataset\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, Flatten\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defining the columns, that is the labels that were newly added to the train_classes via hot encoding.\ncolumns = list(train_classes1.columns[2:]) #from index 2 to the end defines the columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numbers of tags and their names\ncounts = {}\nsplitted_tags = df_train['tags'].map(lambda x: x.split(' '))\nfor labels in splitted_tags.values:\n    for label in labels:\n        counts[label] = counts[label] + 1  if label in counts else 0\n\nplt.figure(figsize=(18, 6))\nplt.title('Classes')\nidxs = range(len(counts.values()))\nplt.xticks(idxs, counts.keys(), rotation=-45)\nplt.bar(idxs, counts.values());","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# random images\nplt.rc('axes', grid = True)\n\n_, ax = plt.subplots(1, 3, figsize=(20, 20))\nrandom_img = np.random.randint(0,len(df_train) - 3)\nfor i , (file, label) in enumerate(df_train[random_img:random_img + 3].values):\n    img = cv2.imread('../input/test-jpg-amazon/test-jpg/{}.jpg'.format(file))\n    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i].set_title('{} - {}'.format(file, label))\n    \nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\nall_labels = splitted_tags.values\nlabels = list(set([y for x in all_labels for y in x]))\n\ndef load_data(df_train, labels, resize):\n    X_train = []\n    y_train = []\n\n    label_map = {l: i for i, l in enumerate(labels)}\n    inv_label_map = {i: l for l, i in label_map.items()}\n\n    for f, tags in df_train.values:\n        img = cv2.imread('input/train-jpg/{}.jpg'.format(f))\n        targets = np.zeros(17)\n        for t in tags.split(' '):\n            targets[label_map[t]] = 1 \n\n        X_train.append(cv2.resize(img,resize))\n        y_train.append(targets)\n        \n    y_train = np.array(y_train, np.uint8)\n    X_train = np.array(X_train, np.float16) / 255.\n\n    return X_train, y_train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = load_data(df_train, labels, resize=(128, 128))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state = int(time.time()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def learning_curve(model_fit, key='acc', ylim=(0.8, 1.01)):\n    plt.figure(figsize=(12,6))\n    plt.plot(model_fit.history[key])\n    plt.plot(model_fit.history['val_' + key])\n    plt.title('Learning Curve')\n    plt.ylabel(key.title())\n    plt.xlabel('Epoch')\n    plt.ylim(ylim)\n    plt.legend(['train', 'test'], loc='best')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fbeta_score_K(y_true, y_pred):\n    beta_squared = 4\n\n    tp = K.sum(y_true * y_pred) + K.epsilon()\n    fp = K.sum(y_pred) - tp\n    fn = K.sum(y_true) - tp\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n\n    result = (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n    return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2)),\n    Dropout(0.1),\n\n    Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2)),\n    Dropout(0.1),\n\n    Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2)),\n    Dropout(0.1),\n\n    Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n    MaxPool2D(pool_size=(2, 2)),\n    Dropout(0.1),\n\n    Flatten(),\n\n    Dense(1024, activation='relu'),\n    Dense(17, activation='sigmoid') \n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[fbeta_score_K])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fit = model.fit(\n    X_train, y_train,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"F beta score: \", score)\nprint(\"Error: %.2f%%\" % (100 - score * 100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(0.003, decay=0.0005)\nbase_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n\nfor layer in base_model.layers:\n    layer.trainable = False\n    \n    model = Sequential([\n    base_model,\n \n    Flatten(), \n        \n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(17, activation='sigmoid')  \n])\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[fbeta_score_K])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fit = model.fit(\n    X_train, y_train,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"Test score (f1): \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decrease learning step and decay\noptimizer = Adam(0.0001, decay=0.00001)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[fbeta_score_K])\n\nmodel_fit = model.fit(\n    X_train, y_train,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"Test score (f1): \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add more layer to learn\nfor layer in model.layers[0].layers[1:]:\n    layer.trainable = True\n\nfor layer in model.layers[0].layers:\n    print(layer.name, layer. trainable)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[fbeta_score_K])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fit = model.fit(\n    X, y,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"F beta score: \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I will check fit_generator for my the best solution\n\naug = keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n                         width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n                         horizontal_flip=True, fill_mode=\"nearest\")\n \nmodel_fit = model.fit_generator(aug.flow(X, y, batch_size=128),\n                        validation_data=(X_val, y_val), steps_per_epoch=len(X) // 128,\n                        epochs=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ny_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"F beta score: \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(0.003, decay=0.0005)\nbase_model = ResNet50(weights=None, include_top=False, input_shape=(128, 128, 3))\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \n\nmodel = Sequential([\n    base_model,\n    \n    Flatten(), \n        \n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(17, activation='sigmoid')\n    \n])\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[fbeta_score_K])\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_fit = model.fit(\n    X_train, y_train,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"F beta score: \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(0.0001, decay=0.00001)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[fbeta_score_K])\nmodel_fit = model.fit(\n    X_train, y_train,\n    batch_size=128,\n    epochs=5,\n    verbose=1,\n    validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"F beta score: \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model.layers[0].layers[1:]:\n    layer.trainable = True\n\nfor layer in model.layers[0].layers:\n    print(layer.name, layer.trainable)\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nmodel_fit = model.fit(\n    X, y,\n    batch_size=128,\n    epochs=3,\n    verbose=1,\n    validation_data=(X_val, y_val))\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_val, batch_size=128)\nscore = fbeta_score(y_val, np.array(y_pred) > 0.2, beta=2, average='samples')\n\nprint(\"Test score (f1): \", score)\nprint(\"Error: %.2f%%\" % (100-score*100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_curve(model_fit, key='loss', ylim=(0, 1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle submission\nX_test = []\nsubmission = []\nfor file in listdir('input/test-jpg'):\n    filename = file.split('.')[0]\n    \n    img = cv2.imread('input/test-jpg/{}.jpg'.format(filename))\n    targets = np.zeros(17)\n    \n    X_test.append(cv2.resize(img, (128, 128)))\n    submission.append(filename)\n\nX_test = np.array(X_test, np.float16) / 255\n\ny_test = model.predict(X_test, batch_size=128)\n\nwith open('understanding_the_amazon_from_space.csv', 'w', newline='') as csvfile:\n    csv_writer = csv.writer(csvfile, delimiter=',',\n                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n    csv_writer.writerow(('image_name', 'tags'))\n    for i, image in enumerate(submission):\n        csv_writer.writerow((image, ' '.join(np.array(labels)[y_test[i] > 0.2])))","metadata":{},"execution_count":null,"outputs":[]}]}